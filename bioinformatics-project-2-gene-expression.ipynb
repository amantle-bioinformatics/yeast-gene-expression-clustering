{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":6740535,"datasetId":3881686,"databundleVersionId":6825184}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Part A: Feature Extraction and Clustering","metadata":{}},{"cell_type":"markdown","source":"An auto-encoder (AE) is a neural network that learns to copy its input to its output. One of the main usage of AEs is to reduce the dimension by extracting meaningful features in the latent space (code layer). Representing data in a lower-dimensional space can improve performance on different tasks, such as classification and clustering.\n1. First, you should construct an AE with 3 neurons in the latent space and train the network with the given dataset.\n2. Then, feed the data to the network and extract features from the latent space.\n3. Implement k-means (with k=3 and 4) and Gaussian mixture model (GMM) clustering methods and compare the results according to the Davies–Bouldin index (DBI) criteria.\n4. Repeat the mentioned steps for network with 5 neurons in the latent space.","metadata":{}},{"cell_type":"code","source":"import csv\nimport numpy as np\nimport keras\nfrom keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import davies_bouldin_score as DBI\n\nspellman_file = open('/kaggle/input/gene-expression-bioinformatics-dataset/Spellman.csv', mode='r')\nspellman_csv = csv.reader(spellman_file)\n\nspellman = []\nfor row in spellman_csv:\n    spellman.append(row)\n\n# Just for example of writing a file: F = open('file', mode='w')\n# Just for example of writing a file: F.write(str(spellman))\n\nspellman = np.array(spellman)\nspellman = spellman[1:, 1:]\nspellman = spellman.astype(float)\n\n# Just for example of writing a code not a module usage: spellman = [x[1:] for x in spellman[1:]]\n# Just for example of writing a code not a module usage: print(spellman)\n\nspell_train, spell_test = train_test_split(spellman, test_size=0.2)\n\nlatent_units = 3\norig_dim = 23\n\ninput_gene = keras.Input(shape=(23,))\nencoded = keras.layers.Dense(latent_units, activation='relu')(input_gene)\ndecoded = keras.layers.Dense(orig_dim, activation='sigmoid')(encoded)\n# Just for a sub-module usage: encoded = layers.Dense(latent_units, activation='relu')(input_gene)\n# Just for a sub-module usage: decoded = layers.Dense(orig_dim, activation='sigmoid')(encoded)\nautoencoder = keras.Model(input_gene, decoded)\n\nencoder = keras.Model(input_gene, encoded)\n\nencoded_input = keras.Input(shape=(latent_units,))\ndecoder_layer = autoencoder.layers[-1]\ndecoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\nautoencoder.fit(spell_train, spell_train,\n                epochs=50,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(spell_test, spell_test))\n\nencoded_genes = encoder.predict(spellman)\nprint(encoded_genes)\n\nprint(\"**************************************************************************\")\nkmeans = KMeans(n_clusters=3, random_state=0).fit(encoded_genes)\nprint(kmeans.labels_)\n\nscore = DBI(encoded_genes, kmeans.labels_)\nprint('score: ' + str(score))\n\nprint(\"##########################################################################\")\nlabels = GaussianMixture(n_components=3, random_state=0).fit_predict(encoded_genes)\nprint(labels)\n\nscore = DBI(encoded_genes, labels)\nprint('score: ' + str(score))\n\n\n# kmeans.labels_ convert to string and then show\n# chideman e mad e nazar, entekhab , format khorooji\n# chetor data chideman bokonid,\n# F = open('file end', mode='w')\n# ghahbl az inke print esh bokonid, bayad data ra bar asase format e madenazar chinesh konid,  va bad write konid.\n# F.write(str(kmeans.labels_))","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:19:10.658489Z","iopub.execute_input":"2024-04-02T07:19:10.658919Z","iopub.status.idle":"2024-04-02T07:19:39.008731Z","shell.execute_reply.started":"2024-04-02T07:19:10.658883Z","shell.execute_reply":"2024-04-02T07:19:39.006863Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Part B: Gene Ontology (GO)","metadata":{}},{"cell_type":"markdown","source":"According to the DBI criteria, select the best result from part A and determine what information can be extracted from each cluster. Use gene ontology (GO) for each identified cluster:\n• Go to the g:Profiler website, http://biit.cs.ut.ee/gprofiler/gost .On the left box, enter your cluster gene names (whitespace-separated)\n• For options, choosing Saccharomyces cerevisiae from organism box.\n• Click Run query button.\n• The results are sorted by p-values in ascending order. Draw a table to list the top 3 GO categories, showing the Term-name, Term-ID, and p-value in each column.","metadata":{}},{"cell_type":"code","source":"import csv\nimport numpy as np\nimport keras\nfrom keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\n\nspellman_file = open('/kaggle/input/gene-expression-bioinformatics-dataset/Spellman.csv', mode='r')\nspellman_csv = csv.reader(spellman_file)\n\nspellman = []\nfor row in spellman_csv:\n    spellman.append(row)\n\nspellman = np.array(spellman)\nspellman = spellman[1:, 1:]\nspellman = spellman.astype(float)\n\n# spellman = [x[1:] for x in spellman[1:]]\n# print(spellman)\n\nspell_train, spell_test = train_test_split(spellman, test_size=0.2)\n\nlatent_units = 3\norig_dim = 23\n\ninput_gene = keras.Input(shape=(23,))\nencoded = keras.layers.Dense(latent_units, activation='relu')(input_gene)\ndecoded = keras.layers.Dense(orig_dim, activation='sigmoid')(encoded)\n# encoded = layers.Dense(latent_units, activation='relu')(input_gene)\n# decoded = layers.Dense(orig_dim, activation='sigmoid')(encoded)\nautoencoder = keras.Model(input_gene, decoded)\n\nencoder = keras.Model(input_gene, encoded)\n\nencoded_input = keras.Input(shape=(latent_units,))\ndecoder_layer = autoencoder.layers[-1]\ndecoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\nautoencoder.fit(spell_train, spell_train,\n                epochs=50,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(spell_test, spell_test))\n\nencoded_genes = encoder.predict(spell_test)\nprint(encoded_genes)\n\nkmeans = KMeans(n_clusters=3, random_state=0).fit(encoded_genes)\nprint(kmeans.labels_)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:19:46.455659Z","iopub.execute_input":"2024-04-02T07:19:46.456826Z","iopub.status.idle":"2024-04-02T07:19:53.919616Z","shell.execute_reply.started":"2024-04-02T07:19:46.456766Z","shell.execute_reply":"2024-04-02T07:19:53.918618Z"},"trusted":true},"outputs":[],"execution_count":null}]}